{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://www.exalumnos.usm.cl/wp-content/uploads/2015/06/Isotipo-Negro.gif\" title=\"Title text\" width=\"20%\" height=\"20%\" />\n",
    "\n",
    "\n",
    "<hr style=\"height:2px;border:none\"/>\n",
    "<h1 align='center'> INF-393 Máquinas de Aprendizaje II-2018 </h1>\n",
    "\n",
    "<H3 align='center'> Tarea 0 - Introducción a Máquinas de Aprendizaje </H3>\n",
    "<hr style=\"height:2px;border:none\"/>\n",
    "\n",
    "**Temas**  \n",
    "* Introducción a librerías comunes de *Machine Learning*:\n",
    "    * Pandas\n",
    "    * Numpy\n",
    "    * Sklearn\n",
    "    * Matplotlib,..\n",
    "* Clasificación y regresión sobre texto.\n",
    "* Implementación a mano de regresión lineal.\n",
    "* Algoritmo Gradiente descendente\n",
    " \n",
    "\n",
    "** Formalidades **  \n",
    "* Equipos de trabajo de: 2 personas\n",
    "* Se debe preparar un (breve) Jupyter/IPython notebook que explique la actividad realizada y las conclusiones del trabajo\n",
    "* Fecha de entrega y discusión: 5 de Octubre.\n",
    "* Formato de entrega: envı́o de link Github al correo electrónico del ayudante (*<francisco.mena.13@sansano.usm.cl>*) , incluyendo al profesor en copia (*<jnancu@inf.utfsm.cl>*). Por favor especificar el siguiente asunto: [Tarea0-INF393-II-2018]\n",
    "\n",
    "<hr style=\"height:2px;border:none\"/>\n",
    "\n",
    "La tarea se divide en secciones:\n",
    "\n",
    "[1.](#primero) Sentiment Analysis  \n",
    "[2.](#segundo) Job Salary Prediction  \n",
    "[3.](#tercero) Linear Regression by hand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"primero\"></a>\n",
    "## 1. Sentiment Analysis [Sin evaluación]\n",
    "El análisis de emociones o sentimientos se refiere al proceso de extraer información acerca de la actitud\n",
    "que una persona (o grupo de ellas) manifiesta, en un determinado medio o formato digital, con respecto a un\n",
    "tópico o contexto de comunicación. Uno de los casos más estudiados corresponde a determinar la polaridad\n",
    "de un trozo de texto, es decir, clasificar una determinada evaluación escrita (*review*), en que una persona\n",
    "manifiesta una opinión, como *positiva*, *negativa* o *neutral*. Esto también ha sido extendido a otros medios, como lo es analizar la polaridad de textos en redes sociales.  \n",
    "\n",
    "<img src=\"https://cdn.urgente24.com/sites/default/files/notas/2018/01/18/twitter.png\" title=\"Title text\" width=\"20%\" height=\"20%\" />\n",
    "\n",
    "\n",
    "La conocida red social Twitter tiene una gran cantidad de usuarios, por lo que la información se genera a\n",
    "cada segundo, donde el análisis de texto se ha aplicado fuertemente a estos medios sociales. La dificultad de\n",
    "este problema radica en el carácter altamente ambiguo e informal del lenguaje que utilizan naturalmente las\n",
    "personas ası́ como el manejo de negaciones, sarcasmo y abreviaciones en una frase.\n",
    "\n",
    "\n",
    "Los datos pueden ser descargados ejecutando el siguiente código en sistema Unix:\n",
    "```\n",
    "wget -O train_data.csv http://www.inf.utfsm.cl/~jnancu/stanford-subset/polarity.train\n",
    "wget -O test_data.csv http://www.inf.utfsm.cl/~jnancu/stanford-subset/polarity.dev\n",
    "```\n",
    "\n",
    "Para construir un clasificador que determine automáticamente la polaridad de un trozo de texto, vamos a necesitar representar los textos $\\{d_i\\}_{i=1}^n$ disponibles como vectores de caracterı́sticas (features). Para aumentar la eficacia de las caracterı́sticas extraı́das es conveniente ejecutar algunas técnicas de pre-procesamiento básicas como: pasar todo el texto a minúsculas (lower-casing), eliminar signos de puntuación\n",
    "y eliminar palabras sin significado como artı́culos, pronombres y preposiciones (*stop word removal*). Otra\n",
    "técnica que suele ser útil para obtener buenas caracterı́sticas (*features*) es la lematización, es decir, la\n",
    "reducción de todas las palabras a su tronco léxico base. Una técnica similar y más utilizada en la práctica es el stemming. Varias de éstas están implementadas en la libreria nltk [13] para python.\n",
    "\n",
    "\n",
    "> a) Construya un dataframe con los datos a analizar. Determine cuántas clases existen, cuántos registros\n",
    "por clase y describa el dataset.\n",
    "``` python\n",
    "import pandas as pd\n",
    "ftr = open(\"train_data.csv\", \"r\")\n",
    "fts = open(\"test_data.csv\", \"r\")\n",
    "rows = [line.split(\" \",1) for line in ftr.readlines()]\n",
    "df_train = pd.DataFrame(rows, columns=['Sentiment','Text'])\n",
    "df_train['Sentiment'] = pd.to_numeric(df_train['Sentiment'])\n",
    "rows = [line.split(\" \",1) for line in fts.readlines()]\n",
    "df_test = pd.DataFrame(rows, columns=['Sentiment','Text'])\n",
    "df_test['Sentiment'] = pd.to_numeric(df_test['Sentiment'])\n",
    "df_train.shape\n",
    "```\n",
    "\n",
    "> b) Construya un conjunto de validación (también conocido como *hold out validation*), a través de una máscara aleatoria, para verificar los resultados de los algoritmos.\n",
    "``` python\n",
    "import numpy as np\n",
    "msk = np.random.rand(len(df_train)) < 0.8\n",
    "df_new_train = df_train[msk]\n",
    "df_val = df_train[~msk]\n",
    "```\n",
    "\n",
    "> c) Implemente y explique un pre-procesamiento para los tweets para dejarlos en un formato estándarizado\n",
    "en el cual se podrán trabajar.\n",
    "```python\n",
    "import re, time\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import WordNetLemmatizer, word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "def word_extractor(text):\n",
    "    wordlemmatizer = WordNetLemmatizer()\n",
    "    wordstemmer = PorterStemmer()\n",
    "    commonwords = stopwords.words('english')\n",
    "    text = re.sub(r'([a-z])\\1+', r'\\1\\1',text)#substitute multiple letter by two\n",
    "    words = \"\"\n",
    "    wordtokens = [ wordstemmer.stem(word.lower())for word in word_tokenize(text.decode('utf-8', 'ignore')) ]\n",
    "    for word in wordtokens:\n",
    "        if word not in commonwords:\n",
    "            words+=\" \"+word\n",
    "    return words\n",
    "word_extractor(\"I love to eat cake\")\n",
    "word_extractor(\"I love eating cake\")\n",
    "word_extractor(\"I loved eating the cake\")\n",
    "word_extractor(\"I do not love eating cake\")\n",
    "word_extractor(\"I don't love eating cake\")\n",
    "...\n",
    "wordlemmatizer.lemmatize(word.lower()) #can use this\n",
    "```\n",
    "\n",
    "> d) Utilizando la función *CountVectorizer* de la librerı́a sklearn genere una representación vectorial del texto de entrenamiento y del conjunto que usaremos para realizar la validación. Esta función consiste en contar cuántas veces aparecen ciertos términos/palabras en el texto a través de un vocabulario que construiremos mediante la unión de todas las palabras que observemos en los textos que tenemos a disposición. Explore el vocabulario utilizado y determine cuáles son las palabras más frecuentes en el conjunto de entrenamiento y validación.\n",
    "``` python\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "texts_train = [word_extractor(text) for text in df_new_train.Text]\n",
    "texts_val = [word_extractor(text) for text in df_val.Text]\n",
    "texts_test = [word_extractor(text) for text in df_test.Text]\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 1), binary='False')\n",
    "vectorizer.fit(np.asarray(texts_train))\n",
    "features_train = vectorizer.transform(texts_train)\n",
    "features_val = vectorizer.transform(texts_val)\n",
    "features_test = vectorizer.transform(texts_test)\n",
    "labels_train = np.asarray((df_new_train.Sentiment.astype(float)+1)/2.0)\n",
    "labels_val = np.asarray((df_val.Sentiment.astype(float)+1)/2.0)\n",
    "labels_test = np.asarray((df_test.Sentiment.astype(float)+1)/2.0)\n",
    "vocab = vectorizer.get_feature_names()\n",
    "dist=list(np.array(features_train.sum(axis=0)).reshape(-1,))\n",
    "for tag, count in zip(vocab, dist):\n",
    "    print count, tag\n",
    "```\n",
    "\n",
    "> e) Construya una función que entrene/ajuste un modelo de Regresión Logı́stica y mida el error de predicción obtenido sobre los datos de entrenamiento y validación. Experimente con variar la representación o preprocesamiento, por ejemplo el efecto de filtrar *stopwords* y de eliminar este paso de\n",
    "pre-procesamiento tı́pico. Determine además, qué representación obtiene un mejor resultado: si aquella obtenida vı́a lematización o aquella obtenida vı́a *stemming*. Seleccione el mejor modelo y mida sobre *test*. Comente\n",
    "``` python\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "def do_LOGIT(x,y,xv,yv):\n",
    "    model = LogisticRegression()\n",
    "    model = model.fit(x, y)\n",
    "    print(\"Accuracy under training: \",model.score(x,y))\n",
    "    print(\"Accuracy under validation: \",model.score(xv,yv))\n",
    "    return model\n",
    "log_model = do_LOGIT(features_train,labels_train,features_val,labels_val)\n",
    "```\n",
    "\n",
    "> f) Finalmente, tome un subconjunto aleatorio de los textos de prueba y analice las predicciones del modelo (explore las predicciones, ası́ como las probabilidades que el clasificador asigna a cada clase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"segundo\"></a>\n",
    "## 2. Job Salary Prediction\n",
    "\n",
    "En esta sección se trabajará con el problema de predecir el salario que ofrece un anuncio en internet a través unicamente del texto del anuncio. El dataset es ofrecido por *Adzuna* como una competencia en la plataforma más grande de *data science* Kaggle, a través del siguiente __[link](https://www.kaggle.com/c/job-salary-prediction)__. El objetivo de la competencia, según *Adzuna*, es el de tener un motor que pueda predecir el salario de cualquier anuncio de trabajo en Reunio Unido, para poder mejorar la experiencia de los usuarios que buscan trabajos, ya que así pueden filtrar sin que el mismo empleador señale explícitamente cuánto paga.\n",
    "\n",
    "<img src=\"http://s5047.pcdn.co/wp-content/uploads/2013/05/salary-prediction-engine-v2.png\" title=\"Title text\" width=\"50%\"/>\n",
    "\n",
    "La métrica de evaluación de la competencia es MAE (*mean absolute error*):\n",
    "\n",
    "$$\n",
    "L = \\frac{1}{N} \\sum_i^N  \\mid y_i - \\hat{y}_i \\mid\n",
    "$$\n",
    "\n",
    "\n",
    "Para descargar los datos a utilizar (*Train_rev1*) debe estar registrado en la plataforma de Kaggle. Se cuenta con cientos de miles registros con textos sin procesar, es decir, no están estructurados.\n",
    "\n",
    "\n",
    "> a) Carge los datos *csv* de entrenamiento y cree un conjunto de validación con los últimos 10 mil datos en un dataframe de *pandas*. Describa los datos, apóyese de gráficos ¿Cuántos datos hay en cada conjunto?  \n",
    "```python\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"Train_rev1.csv\")\n",
    "df_train = df.iloc[:-10000]\n",
    "df_val = df.iloc[-10000:]\n",
    "...#load other sets\n",
    "df.head()\n",
    "```\n",
    "*Recuerde que si no puede trabajar con todos los datos, debido a su volumen, puede muestrear*.\n",
    "\n",
    "\n",
    "> b) Extraiga los datos de cada conjunto con los que trabajará, el *input* $X$, los textos, y el *output* $y$, los salarios.\n",
    "```python\n",
    "text = df.FullDescription\n",
    "salary = df.SalaryNormalized\n",
    "```\n",
    "\n",
    "> c) Realice un pre-procesamiento a los datos brutos de texto para extraer características y generar la representación de los datos de entrada al modelo $\\vec{x}$. Comente sobre lo realizado.\n",
    "\n",
    "> d) Intente resolver el problema enfrentándolo como regresión con el modelo de regresión lineal ordinaria en *sklearn*. ¿Qué es lo que hace *fit_intercept=True*? Evalúe la función objetivo (F.O.) utilizada y la métrica de la competencia (*mean absolute error*) en ambos conjuntos generados en el punto a). Comente lo observado.\n",
    "```python\n",
    "from sklearn.linear_model import LinearRegression as LR\n",
    "model = LR(fit_intercept=True, normalize=False)\n",
    "model.fit(X_train,y_train)\n",
    "... #measure F.O.\n",
    "from sklearn.metrics import mean_absolute_error #measure MAE\n",
    "print(\"MAE on train: \",mean_absolute_error(y_train, model.predict(X_train)))\n",
    "print(\"MAE on validation: \",mean_absolute_error(y_val, model.predict(X_val)))\n",
    "```\n",
    "\n",
    "> e) Intente mejorar sus resultados en base a la métrica de la competencia (MAE) sobre el conjunto de validación. Comente sobre lo realizado.  \n",
    "*No se le pide que imite los resultados ganadores de la competencia (MAE de 3400), sino que mejore lo ya alcanzado siendo creativo.*\n",
    "<div class=\"alert alert-warning\"> HINT: Una opción es cambiar el *approach* de resolución desde regresión a casificación o trabajo sobre los datos (tal como limpiarlos).</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import re, time\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import WordNetLemmatizer, word_tokenize\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Train_rev1.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El dataset se compone de 12 atributos, los cuales son:\n",
    "\n",
    "+ Id: identificador único para cada anuncio laboral.\n",
    "+ Title: título del aviso laboral.\n",
    "+ FullDescription: texto completo del anuncio laboral.\n",
    "+ LocationRaw: ubicación del empleo.\n",
    "+ LocationNormalized: ubicación normalizada.\n",
    "+ ContractType: tipo de contrato.\n",
    "+ ContractTime: duración del contrato.\n",
    "+ Company: nombre del empleador. \n",
    "+ Category: categoría del empleo.\n",
    "+ SalaryRaw: salario.\n",
    "+ SalaryNormalized: salario anual.\n",
    "+ SourceName: nombre del sitio o anunciante que provee el aviso laboral."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El dataset contiene 244.768 registros, cada uno con 12 atributos distintos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Del resumen del dataset se puede observar que ningún atributo cuenta con registros nulos. Además, dos de los atributos son de tipo númerico y el resto de los atributos son textos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De los atributos númericos, el que nos interesa para las predicciones es *SalaryNormalized*. El método `describe()` muestra las estadísticas para dicho atributo.\n",
    "\n",
    "Para estudiar el comportamiento del atributo *SalaryNormalized* se realiza un boxplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(list(df.SalaryNormalized))\n",
    "plt.ylabel('Datos')\n",
    "plt.xlabel('SalaryNormalized')\n",
    "plt.title('Salario')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es evidente que los datos presentan un sesgo negativo bastante significativo. Tambien se aprecia la presencia de muchos outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para un primer acercamiento al problema, el dataset de entrenamiento se separará en un conjunto de entrenamiento con 15.000 registros y un conjunto de validación con 10.000 registros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = np.random.choice(df.index.values, 15000, replace=False) #Elige 15000 filas al azar del df, no se repiten.\n",
    "\n",
    "df_train = df.loc[rows]\n",
    "df_val = df.iloc[-10000:]\n",
    "\n",
    "print(\"Largo entrenamiento:\", len(df_train.index), \"|\", \"Largo validación:\", len(df_val.index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> b) Extraiga los datos de cada conjunto con los que trabajará, el *input* $X$, los textos, y el *output* $Y$, los salarios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, se separan los conjuntos de entrenamiento y validación en los respectivos inputs y outputs. Siendo el input $X$ el atributo *FullDescription* y el output $Y$ el atributo *SalaryNormalized*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texto_train = df_train.FullDescription\n",
    "salario_train = df_train.SalaryNormalized\n",
    "\n",
    "texto_val = df_val.FullDescription\n",
    "salario_val = df_val.SalaryNormalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> c) Realice un pre-procesamiento a los datos brutos de texto para extraer características y generar la representación de los datos de entrada al modelo $\\vec{x}$. Comente sobre lo realizado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para pre-procesar el texto se decidió utilizar la lematización debido a que considera el contexto de las palabras y puede resultar útil para el proceso de aprendizaje. Tambien se filtran las stopwords o palabras que no aportan significado al texto, mediante expresiones regulares se eliminan patrones como \\*\\*\\*\\* entre otros y se convierten las palabras a minúsculas. Por último, debido a la naturaleza del problema, se decide omitir las palabras cuyo largo sea mayor a 20 carácteres.\n",
    "\n",
    "A continuación, se define la función `word_extractor` encargada del pre-procesamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "palabrasComunes = stopwords.words('english')\n",
    "\n",
    "def word_extractor(text, commonwords):\n",
    "    wordlemmatizer = WordNetLemmatizer()\n",
    "    text = re.sub('\\\\*\\\\*\\\\*\\\\*|\\\\.|\\\\,|\\\\/|\\\\|', '',text) #elimina los **** del texto\n",
    "    words = \"\"\n",
    "    wordtokens = [ wordlemmatizer.lemmatize(word.lower())for word in word_tokenize(text) ]\n",
    "    \n",
    "    for word in wordtokens:\n",
    "        if word not in commonwords and len(word) < 20:\n",
    "            words+=\" \"+word\n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se construyen las estructuras que serán utilizadas para el proceso de aprendizaje del modelo usando la función definida anteriormente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "texts_train = [word_extractor(text, palabrasComunes) for text in texto_train]\n",
    "texts_val = [word_extractor(text, palabrasComunes) for text in texto_val]\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 1), binary='False')\n",
    "vectorizer.fit(np.asarray(texts_train))\n",
    "\n",
    "features_train = vectorizer.transform(texts_train)\n",
    "features_val = vectorizer.transform(texts_val)\n",
    "\n",
    "labels_train = np.asarray(salario_train)\n",
    "labels_val = np.asarray(salario_val)\n",
    "\n",
    "print(\"Tiempo que tarda en ejecutarse:\", round(time.time()-start, 1), \"segundos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se tiene una matriz dispersa para el input de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = vectorizer.get_feature_names()\n",
    "dist=list(np.array(features_train.sum(axis=0)).reshape(-1,))\n",
    "lista = zip(vocab, dist)\n",
    "lista_ordenada = sorted(lista, key=lambda x: x[1], reverse=True)\n",
    "lista_ordenada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la celda anterior se muestran las palabras más frecuentes del vocabulario generado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> d) Intente resolver el problema enfrentándolo como regresión con el modelo de regresión lineal ordinaria en *sklearn*. ¿Qué es lo que hace *fit_intercept=True*? Evalúe la función objetivo (F.O.) utilizada y la métrica de la competencia (*mean absolute error*) en ambos conjuntos generados en el punto a). Comente lo observado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se procede a trabajar con un modelo de regresión lineal ordinaria. Para esto se utiliza el método `LinearRegression()` con parámetros `fit_intercept = True` y `normalize = False`. *fit_intercept* determina si se calcula el intercepto $\\beta_0$ de la regresión lineal y solo debiese ser falso si se espera que los datos estén previamente centrados, por otra parte,*normalize* determina si se normalizan los regresores $X$ antes de realizar la regresión lineal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "model = LinearRegression(fit_intercept=True, normalize=False)\n",
    "model.fit(features_train, labels_train)\n",
    "\n",
    "#measure MAE\n",
    "print(\"MAE sobre entrenamiento: \", round(mean_absolute_error(labels_train, model.predict(features_train)), 1))\n",
    "print(\"MAE sobre validación: \", round(mean_absolute_error(labels_val, model.predict(features_val)), 1))\n",
    "\n",
    "print(\"Tiempo que tarda en ejecutarse:\", round(time.time()-start, 1), \"segundos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La métrica utilizada para evaluar el rendimiento del modelo corresponde al Error Absoluto Medio (MAE), cuya fórmula es:\n",
    "\n",
    "$$\n",
    "L = \\frac{1}{N} \\sum_i^N  \\mid y_i - \\hat{y}_i \\mid\n",
    "$$\n",
    "\n",
    "Se interpreta como la distancia media entre las predicciones y los valores reales. En este caso, para el conjunto de validación se obtuvo un MAE bastante alto. Este valor puede deberse a usar pocos datos para el entrenamiento y/o un pre-procesamiento que no es adecuado.\n",
    "\n",
    "En el siguiente inciso se propone una serie de cambios para mejorar el rendimiento del modelo y se discute el resultado obtenido."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> e) Intente mejorar sus resultados en base a la métrica de la competencia (MAE) sobre el conjunto de validación. Comente sobre lo realizado.  \n",
    "*No se le pide que imite los resultados ganadores de la competencia (MAE de 3400), sino que mejore lo ya alcanzado siendo creativo.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los cambios que se proponen consisten en:\n",
    "+ Eliminar los outliers para el conjunto de entrenamiento (el conjunto de validación se mantiene igual que en los incisos anteriores).\n",
    "+ Eliminar palabras/símbolos que podrían estar generando ruido.\n",
    "\n",
    "Primero se carga el dataset nuevo omitiendo los registros cuyo *SalaryNormalized* sea mayor a 68000. Este numero se obtiene sel boxplot realizado en los incisos anteriores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nuevo = df[df.SalaryNormalized <= 68000]\n",
    "df_nuevo.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, se construye nuevamente el boxplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(list(df_nuevo.SalaryNormalized))\n",
    "plt.ylabel('Datos')\n",
    "plt.xlabel('SalaryNormalized')\n",
    "plt.title('Salario sin outliers')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se puede apreciar, no queda ningun outlier y la distribución de los datos se asemeja más a una normal. Esto facilitará el proceso de aprendizaje del modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego, se repite el procedimiento realizado en los incisos anteriores usando el nuevo dataset para evaluar si existe una mejora respecto al primer modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_nuevo = np.random.choice(df_nuevo.index.values, 15000, replace=False) #Elige 15000 filas al azar del df_nuevo, no se repiten.\n",
    "\n",
    "df_train_nuevo = df_nuevo.loc[rows_nuevo] ##el conjunto de entrenamiento se obtiene del dataset sin outliers\n",
    "df_val_nuevo = df.iloc[-10000:] ##El conjunto de validación es el mismo que el anterior\n",
    "\n",
    "print(\"Largo entrenamiento:\", len(df_train_nuevo.index), \"|\", \"Largo validación:\", len(df_val_nuevo.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texto_train_nuevo = df_train_nuevo.FullDescription\n",
    "salario_train_nuevo = df_train_nuevo.SalaryNormalized\n",
    "\n",
    "texto_val_nuevo = df_val_nuevo.FullDescription\n",
    "salario_val_nuevo = df_val_nuevo.SalaryNormalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "palabrasComunes = stopwords.words('english')\n",
    "palabrasRuido = [\":\", \".\", \",\", \";\", \"12\", \"10\", ]\n",
    "\n",
    "def word_extractor2(text, commonWords, noiseWords):\n",
    "    wordlemmatizer = WordNetLemmatizer()\n",
    "    text = re.sub('\\\\*\\\\*\\\\*\\\\*|\\\\/|\\\\|', '',text)\n",
    "    words = \"\"\n",
    "    wordtokens = [ wordlemmatizer.lemmatize(word.lower())for word in word_tokenize(text) ]\n",
    "    \n",
    "    for word in wordtokens:\n",
    "        if word not in commonWords and word not in noiseWords and len(word) < 20:\n",
    "            words+=\" \"+word\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "texts_train_nuevo = [word_extractor2(text, palabrasComunes, palabrasRuido) for text in texto_train_nuevo]\n",
    "texts_val_nuevo = [word_extractor2(text, palabrasComunes, palabrasRuido) for text in texto_val_nuevo]\n",
    "\n",
    "vectorizer2 = CountVectorizer(ngram_range=(1, 1), binary='False')\n",
    "vectorizer2.fit(np.asarray(texts_train_nuevo))\n",
    "\n",
    "features_train_nuevo = vectorizer2.transform(texts_train_nuevo)\n",
    "features_val_nuevo = vectorizer2.transform(texts_val_nuevo)\n",
    "\n",
    "labels_train_nuevo = np.asarray(salario_train_nuevo)\n",
    "labels_val_nuevo = np.asarray(salario_val_nuevo)\n",
    "\n",
    "print(\"Tiempo que tarda en ejecutarse:\", round(time.time()-start, 1), \"segundos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "model_nuevo = LinearRegression(fit_intercept=True, normalize=False)\n",
    "model_nuevo.fit(features_train_nuevo, labels_train_nuevo)\n",
    "\n",
    "#measure MAE\n",
    "print(\"MAE sobre entrenamiento: \", round(mean_absolute_error(labels_train_nuevo, model_nuevo.predict(features_train_nuevo)), 1))\n",
    "print(\"MAE sobre validación: \", round(mean_absolute_error(labels_val_nuevo, model_nuevo.predict(features_val_nuevo)), 1))\n",
    "\n",
    "print(\"Tiempo que tarda en ejecutarse:\", round(time.time()-start, 1), \"segundos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En general, se obtiene una mejora considerable respecto al modelo anterior, llegando a haber una diferencia de 6000 en el MAE. Sin embargo, existen ocasiones en que el primer modelo obtiene mejores resultados que el segundo.\n",
    "\n",
    "La idea de eliminar los outliers es hacer que el modelo sea mas estable, realizando así mejores predicciones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"tercero\"></a>\n",
    "## 3. Linear Regression by hand\n",
    "\n",
    "En esta sección se le pedirá que implemente la regresión lineal ordinaria a través del algoritmo SGD (*Stochastic Gradient Descend*) para encontrar los parámetros de la regresión a través de este algoritmo de manera iterativa. La técnica de SGD es sin duda dominante al momento de entrenar modelos en máquinas de aprendizaje cuando la solución no tiene un óptimo derivable analíticamente, en este caso la regresión lineal ordinaria que trabajaremos si tiene óptimo anaĺitico, sin embargo, se le pedirá que compare este caso con fines pedagógicos.\n",
    "\n",
    "* Regresión lineal ordinaria:\n",
    "$$\n",
    "\\hat{y} = f(\\vec{x}) =\\vec{\\beta}^T\\cdot \\vec{x}\n",
    "$$\n",
    "\n",
    "* Función objetivo:\n",
    "$$\n",
    "Loss = \\frac{1}{N} \\sum_i^N ( y_i - \\hat{y}_i )^2\n",
    "$$\n",
    "\n",
    "\n",
    "* Algoritmo SGD para regresión lineal ordinaria:\n",
    "$$ \\vec{\\beta}^{(t+1)} \\leftarrow \\vec{\\beta}^{(t)} - \\eta \\nabla_{\\vec{\\beta}^{(t)}} Loss $$\n",
    "\n",
    "Para lo que sigue de la actividad sólo podrá utilizar *numpy* (para operaciones de algebra lineal).\n",
    "\n",
    "> a) Escriba una función que calcule la función de pérdida, error cuadrático medio (MSE - *mean squared error*), para un dato o para un conjunto de datos.\n",
    "\n",
    "> b) Escriba una función que calcule el gradiente (derivada) de la función de pérdida anterior, para un dato o para un conjunto de datos. *Escriba explícitamente la derivada (gradiente)*.\n",
    "\n",
    "> c) Escriba una función que calcule los parámetros de una regresión lineal simple de manera analítica (es decir el mínimo global). \n",
    "\n",
    "<div class=\"alert alert-warning\"> Hint: ésto debería ser la ecuación desglosada de la derivada igual a 0, la cual incluye $(X^TX)^{-1}$</div>\n",
    "\n",
    "\n",
    "> d) Ahora escriba un programa que permita entrenar una regresión lineal a través del algoritmo SGD mostrado en la ecuación del algoritmo SGD, es decir, que de manera iterativa, vaya tomando un dato a la vez, y actualizando el parámetro $\\beta$ a través del gradiente descendiente de la función de pérdida de la regresión lineal ordinaria, de la pregunta b).\n",
    "\n",
    "> e) Demuestre que sus programas funcionan en un problema de regresión simple. Para esto utilice el dataset **Boston Housing** , disponible a través de la librería __[*sklearn*](http://scikit-learn.org)__, el cual corresponde a el precio de diferentes casas en Boston además de distintas características relevantes respecto al lugar, como por ejemplo el crimen en la ciudad, el número de habitaciones, que tan vieja es, distancia a lugares relevantes, entre otros. Éstas características deben combinarse linealmente para estimar el precio de la casa.\n",
    "<div class=\"alert alert-block alert-info\">Es una buena práctica el normalizar los datos antes de trabajar con el modelo</div>\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import load_boston\n",
    "X_train,y_train = load_boston(return_X_y=True)\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "```\n",
    "Para evaluar los resultados, construya un gráfico correspondiente a la función de pérdida utilizada en el entrenamiento *versus* número de iteraciones (**realice 1000 iteraciones**), utilizando sólo el conjunto de entrenamiento (el objetivo de esta sección es familiarizarse con el algoritmo). Además de reportar el tiempo de entrenamiento mediante el algoritmo implementado en c) y d).\n",
    "\n",
    "> e) Varié la tasa de aprendizaje $\\eta \\in [0,1]$ del algoritmo SGD del punto d), compare los resultados entre sí y con la solución óptima encontrada en c). Comente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para realizar la pregunta a, se importa la librería numpy. La función que se genera es lossFunction, la cual recibe como parámetro \"y\" que es un np.array que contiene los valores exactos de la función f(x) y \"y_prima\" es un np.array con la aproximación de la función f(x)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "def lossFunction(y, y_prima):\n",
    "    resta = y - y_prima\n",
    "    restaCuadrado = np.power(resta, 2)\n",
    "    suma = np.sum(restaCuadrado)\n",
    "    return suma / y.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para la pregunta b, se define la función LossFunctionGradient, la cual recive como parámetro el vector columna \"y\", eñ vectpr columna \"b\" que posee los coeficientes de la regresión lineal y \"X\" una matriz de NxP, donde N es la cantidad de inputs y P es la cantidad de parámetros del input más 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2588.        ],\n",
       "       [1252.66666667],\n",
       "       [2952.        ],\n",
       "       [ 384.        ]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def LossFunctionGradient(y, b, X):\n",
    "    # Arreglo donde se guardará el gradiente\n",
    "    gradiente = np.array([0.0])\n",
    "    # Número de ejemplos\n",
    "    n = y.size\n",
    "    # El primer for itera para obtener la derivada de cada variable B\n",
    "    for i in range(b.size):\n",
    "        # Variable donde se almacena el gradiente por partes de una variable de B\n",
    "        gradParcial = 0\n",
    "        # Este for es para recorrer todos los componentes de X y considerarlos en la derivada\n",
    "        for c in range(n):\n",
    "            gradParcial += (-2 / n) * (y[c] - np.dot(X[c], b))* X[c][i]\n",
    "            \n",
    "        if i == 0:\n",
    "            gradiente[0] = gradParcial\n",
    "        else:\n",
    "            gradiente = np.append(gradiente, gradParcial)\n",
    "    # Se retorna el gradiente de B\n",
    "    return gradiente.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para la pregunta c, se utiliza el resultado de la derivada igualada a 0: $(X^{T}X)^{-1}X^{T}Y = B$ para generar la función factoresB que recibe como parámetro el vector Y y la matriz X. Esto se logra a partir de las funciones de numpy que permiten obtener la transpuesta de una matriz, la inversa y poder multiplicar matrices. El retorno de esta función es igual a un vector columna que posee los factores de B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.140625  ]\n",
      " [-3.14635768]\n",
      " [ 4.65625   ]\n",
      " [-9.84375   ]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 3,  4,  5,  1],\n",
       "       [ 6,  7,  8,  1],\n",
       "       [ 9, 19, 11,  1]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def factoresB(Y, X):\n",
    "    XT = X.transpose()\n",
    "    # (X_T * X)^(-1)\n",
    "    mult = np.dot(XT, X)\n",
    "    inv = np.linalg.inv(mult)\n",
    "    mult = np.dot(inv, XT)\n",
    "    mult = np.dot(mult, Y)\n",
    "    return mult"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para la pregunta d, se realiza una función que obtiene los coeficientes de B mediante SGD. Esta recibe como parámetro el vector columna y, la matriz X de dimensiones NxP, los valores iniciales del vector columna B, n que es la tasa de aprendizaje que por defecto es 0.001 y N que es la cantidad de iteraciones de la aproximación. Esta función se apoya en la función realizada anteriormente para calcular el gradiente de B (LossFunctionGradient)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[21579.93890811],\n",
       "       [39647.67162265],\n",
       "       [27587.38189112],\n",
       "       [ 3011.2214915 ]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def SGD(y, x, B, n = 0.001, N = 10):\n",
    "    for i in range(N):\n",
    "        B = B - n * LossFunctionGradient(y, B, x)\n",
    "    return B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para la pregunta e, se utiliza el dataset load_boston que contiene varios parámetros que determinan el precio de una propiedad. Primero se normalizan este dataset para tener una mejor estimación y que no se vea afectada por la diferencia de escalas. Luego, a los valores de entrenamiento X, se le agrega una columna de 1 al final para que se corresponda con la regresión lineal. Además, se genera un vector de coeficientes iniciales iguales a 0. Se ajusta un valor de tasa de entrenamiento igual a 0.0007. Luego, se utiliza la función SGD para obtener las estimaciones de los coeficientes de la regresión lineal. Esta se ocupa desde 1 a 1000 iteraciones y se guardan los resultados para así obtener los valores de y estimados. Con los valores de y_estimados e y reales, se utiliza la función lossFunction para cualcular el error cuadrático. Con esto se logra determinar el error para cada iteración y así graficarla. Además, se utiliza la librería Time para poder guardar el tiempo de computo de cada iteración."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.04211115837097168\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "print(end - start)\n",
    "X_train,y_train = load_boston(return_X_y=True)\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "\n",
    "# Se agrega columna de unos al final\n",
    "X_train = np.c_[X_train, np.ones(506)]\n",
    "# Vector inicial\n",
    "B = np.array(([[0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0]]))\n",
    "\n",
    "y_estimado = list()\n",
    "tiempo = np.array([0.0])\n",
    "# Primera iteración\n",
    "start = time.time()\n",
    "b = SGD(y_train, X_train, B, N = 1, n = 0.0007)\n",
    "end = time.time()\n",
    "tiempo[0] = end - start\n",
    "\n",
    "y_estimado.append(np.dot(X_train, b))\n",
    "\n",
    "for i in range(2, 1001):\n",
    "    start = time.time()\n",
    "    b = SGD(y_train, X_train, B, N = i, n = 0.0007)\n",
    "    end = time.time()\n",
    "    np.append(tiempo, (end - start))\n",
    "    y_estimado.append(np.dot(X_train, b))\n",
    "    \n",
    "#n = 0.0007\n",
    "#print(np.dot( X_train[10],SGD(y_train, X_train, B, N = 1000, n = 0.0007)))\n",
    "#y_train\n",
    "print(y_estimado)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para la pregunta f se genera un widget que permite cambiar la tasa de aprendizaje."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9563cc176484006a5bc5a406ab81629",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatLogSlider(value=0.7943282347242815, continuous_update=False, description='Tasa', ma…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.widgetfunc(valor)>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ipywidgets import widgets, fixed\n",
    "def widgetfunc(valor):\n",
    "    Betas = SGD(y_train, X_train, B, N = 1000, n = valor)\n",
    "    print(Betas)\n",
    "    \n",
    "sensibilidad = widgets.FloatLogSlider(\n",
    "    value=10,\n",
    "    base=10,\n",
    "    min=-4, # max exponent of base\n",
    "    max=-0.1, # min exponent of base\n",
    "    step=0.1, # exponent step\n",
    "    description='Tasa',\n",
    "    continuous_update=False\n",
    "    \n",
    ")\n",
    "widgets.interact(widgetfunc, valor = sensibilidad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como conclusión final de lo realizado, la convergencia del método depende mucho de la tasa de aprendizaje, es decir, que si se elige mal la tasa de aprendizaje no se podrá obtener una aproximación útil. En este caso, la tasa de aprendizaje converge cuando el valor de esta es pequeño (alrededor de 0.0007) pero si se hace muy pequeño, el método comienza de divergir y nuevamente no converge. También este método depende del valor inicial que se le de a B al comienzo del algoritmo. Si se elige uno inadecuado, puede que este no converja nunca nuevamente. Finalmente, este es un método útil cuando se sabe cual es la mejor tasa de entrenamiento y además de conocer aproximadamente que B es útil para que el método converja."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Referencias\n",
    "[1] Hastie, T.; Tibshirani, R., Friedman, J. (2009), *The Elements of Statistical Learning*, Second Edition.\n",
    "Springer New York Inc.  \n",
    "[2] http://statweb.stanford.edu/tibs/ElemStatLearn/datasets/  \n",
    "[3] https://en.wikipedia.org/wiki/Stopwords  \n",
    "[4] https://en.wikipedia.org/wiki/Stemming  \n",
    "[5] https://en.wikipedia.org/wiki/Lemmatisation  \n",
    "[6] http://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature extraction.text  \n",
    "[7] http://www.nltk.org/  \n",
    "[8] https://gluon.mxnet.io/chapter06_optimization/gd-sgd-scratch.html  \n",
    "[9] https://towardsdatascience.com/linear-regression-simplified-ordinary-least-square-vs-gradient-descent-48145de2cf76   \n",
    "[10] https://en.wikipedia.org/wiki/Numerical_differentiation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
